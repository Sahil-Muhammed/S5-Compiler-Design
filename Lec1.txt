Why study compiler design?

	- Its everywhere
	- Applications are numerous: parser for HTML in web browser, interpreters for JS/Flash, Machine code generation for high level languages, software testing, program optimization, malicious code detection, design of new computer architectures (compiler-in-the-loop hardware development), hardware synthesis: VHDL to RTL translation, Compiler simulation

Complexity of Compiler Tech

	- Possibly most complex system software
	- Writing it is a substantial exercise in software engineering
	- Complexity arises from the fact that it is required to map a programmer's requirements (in a HLL program) to architectural details
	- Uses algorithms and techniques from a very large number of areas in CS

Nature of Compiler Algs
	
	- Draws results from mathematical logic, lattice theory, linear algebra, probability, etc: type checking, static analysis, dependence analysis, loop parallelization, cache analysis, etc.
	- Practical application:
		- Greedy alg - register allocation
		- Heuristic search - list scheduling
		- Graph algorithms - dead code elimination, register allocation
		- Dynamic programming - instruction selection
		- Optimization techniques - instruction scheduling
		- Finite automata - lexical analysis
		- Pushdown automata - parsing
		- Fixed point algs - data-flow analysis

Other Uses of Scanning and Parsing Techniques

	- Assembler implementation
	- Online text searching (GREP, AWK) and word processing
	- Website filtering
	- Command language interpreters
	- Scripting language interpretation (Unix shell, Perl, Python)

Other Uses of Program Analysis Techniques

	- Converting a sequential loop to a parallel loop
	- Program analysis to determine if programs are data-race free (are there 2 threads accessing the same resource?)
	- Profiling programs to determine busy regions
	- Program slicing
	- Data-flow analysis approach to software testing: uncovering errors along all paths, dereferencing null pointers, buffer overflows and memory leaks.
	- Worst Case Executino Time (WCET) estimation and energy analysis (computation of amount of energy that the program takes)


Language Processing System

	- source program --> Preprocessor (expands macros) --> modified source program --> Compiler --> Target Assembly program --> Assembler --> Relocatable Machine Code --> Linker/Loader --> Target Machine Code


Compiler Overview

	- char stream --> Lexical Analysis --> Token stream --> Syntax Analyzer --> Semantic Analyzer --> Annotated Syntax Tree --> Intermediate Code Generator --> Intermediate Representation ...


Compilers and Interpreters

- 7 blocks along with symbol table.
- Interpreter stops after intermediate code generator

- Compiler generate machine code, interpreters interpret intermediate code
- Interpreters are easier to write and can provide better error messages (symbol table is still available)
- Interpreters are at least 5 times slower than machine code generated by compilers
- Interpreters also require much more memory than machine code generated by compilers
- Examples: Perl, Python, Unix Shell, Java, BASIC, LISP --> Interpreter based languages


Translation Overview - Lexical Analysis

- fahrenheit = centigrade * 1.8 + 32 --> Lexical Analysis --> {<id, 1> <assign> <id, 2> <multop> <fconst, 1.8> <addop> <iconst, 32> --> Syntax Analyzer


Lexical Analysis

- LA can be generated automatically from regular expression specifications
	- LEX and Flex are two such tools
- LA is a deterministic finite state automaton
- Why is LA separate from parsing?
	- Simplification of design - software engineering reason
	- I/O issues are limited LA alone
	- LA based on finite automata are more efficient to implement than pushdown automata used for parsing (due to stack)

Translation Overview - Syntax Analysis

- <id, 1> <assign> <id,2> <multop> <fconst, 1.8> <addop> <iconst, 32> --> Syntax Analyzer --> (Abstract) Syntax Tree --> Semantic Analyzer

Parsing or Syntax Analysis

- Syntax analyzers (parsers) can be generated automatically from several variants of context-free grammar specifications:
	- LL(1), and LALR(1) are the most popular ones
	- ANTLR (for LL(1)), YACC and Bison (for LALR(1)) are such tools
- Parsers are deterministic push-down automata
- Parsers cannot handle context-sensitive features of programming languages
	- Ex: Variables are declared before use, types match on both sides of assignments, parameter types and number match in declaration and use.

Semantic Analysis

- Semantic consistency that cannot be handled at the parsing stage is handled here
- Type checking of various programming language constructs is one of the most important tasks
- Stores type information in the symbol table or the syntax tree
	- Types of variables, function parameter, array dimensions, etc.
	- Used not only for semantic validation but also for subsequent phases of compilation
- Static semantics of programming languages can be specified using attribute grammars

Translation Overview - Intermediate Code Generation

- Annotated syntax tree (output from semantic analyzer) --> Intermediate Code Generator --> Basic target language instructions --> Code optimizer


Intermediate Code Generation

- While generating machine code directly from source code is possible, it entails two problems
	- With m languages and n target machines, we need to write m x n compilers
	- The code optimizer which is one of the largest and very-difficult-to-write components of any compiler cannot be reused.
- By converting source code to an intermediate code, a machine-independent code optimizer may be written
- Intermediate code must be easy to produce and easy to produce and easy to translate to machine code
	- A sort of universal assembly language
	- Should not contain any machine-specific parameters (registers, addresses, etc.)
