Regular Expressions

- Let \Sigma be an alphabet. The regular expressions over \Sigma and the languages they denote (or generate) are defined as below:

	- \phi is an RE. L(\phi) = \phi
	- \epsilon is an RE. L(\epsilon) = {\epsilon}
	- For each a \in \Sigma, a is an RE. L(a) = {a}
	- If r and s are REs denoting the languages R and S, respectively
		- (rs) is an RE, L(rs) = R.S = {xy | x \in R AND y \in S}
		- (r+s) is an RE, L(r+s) = R U S
		- (r^*) is an RE, L(r^*) = R^* = U_{i = 0}^{\infty} R^i
		- (L* is called the Kleene closure)

Examples of Regular Expressions

- L = set of all strings of 0s and 1s
	- r = (0 + 1)^*
		- How to generate 101?
		- (0 + 1)^* ==>^4 (0 + 1)(0 + 1)(0 + 1)\epsilon ==>^4 101
- L = set of all strings of 0s and 1s, with at least two consecutive 0s
	- r = (0 + 1)^*00(0 + 1)^*
- L = {w \in {0, 1}^* | w has two or three occurrences of 1, the first and second of which are not consecutive}
	- r = 0^*10^*010^*(10^* + \epsilon)
- r = (1 + 10)^*
	- L = set of all strings of 0s and 1s, beginning with 1 and not having two consecutive 0s
- r = (0 + 1)^*011
	- L = set of all strings of 0s and 1s ending in 011
- r = c^*(a + bc^*)^*
	- L = set of all strings over {a, b, c} that do not have the substring ac
- L = {w | w \in {a, b}^* AND w ends with a }
	- r = (a+b)^*a
- L = {if, then, else, while, do, begin, end}
	- r = if + then + else + while + do + begin + end

Examples of Regular Definitions

- A regular definition is a sequence of "equations" of the form d_1 = r_1; d_2 = r_2; ...; d_n = r_n, where each d_i is a distinct name, and each r-I is a regular expression over the symbols \Sigma U {d_1, d_2, ..., d_i-1}

- identifiers and integers
	- letter = a + b + c + d + e; digit = 0 + 1 + 2 + 3 + 4;
	- identifier = letter(letter + digit)^*; number = digit digit^*
- unsigned numbers
	- digit = 0 + 1 + 2 + 3 + 4 + 5 + 6 + 7 + 8 + 9;
	- digits = digit digit^*;
	- optional_fraction = digits + \epsilon;
	- optional_exponent = (E(+|-|\epsilon)digits) + \epsilon
	- unsigned_number = digits optional_fraction optional_exponent

Equivalence of REs and FSA

- Let r be an RE. Then there exists an NFA with \epsilon - transitions that accepts L(r). The proof is by construction.
- If L is accepted by a DFA, then L is generated by an RE. The proof is tedious.

Transition Diagrams

- Transition diagrams are generalized DFAs with the following differences
	- Edges may be labelled by a symbol, a set of symbols, or a regular definition
	- Some accepting states may be indicated as retracting states, indicating that the lexeme does not include the symbol that brought us to the accepting state.
	- Each accepting state has an action attached to it, which is executed when that state is reached. Typicallly, such an action returns a token and its attribute value.
- Transition diagrams are not meant for machine translation, but only for manual translation.

Transition diagram for identifiers and reserved words

- States            Symbol
-         Letter    Digit    Other
-   q_0     q_1      
-   q_1     q_1      q_1      q_2
-   q_2    

Intermediate Code Generation

- While generating machine code directly from source code is possible, it entails two problems:
	- WIth m languages and n target machines, we need to write m x n compilers.
	- The code optimizer which is one of the largest and very-difficult-to-write components any compiler cannot be reused
- By converting source code to an intermediate code, a machine-independent code optimizer may be written.
- Intermediate code must be easy to produce and easy to translate to machine code
	- A sort of universal assembly language
	- Should not contain any machine-specific parts (registers, addresses, etc.)

Different Types of Intermediate Code

- The type of intermediate code deployed is based on the application
- Quadruples, triples, indirect triples, abstract syntax trees are the classical forms used ofr machine-independent optimizations and machine code generation
- Static Single Assignment form (SSA) is a recent form and enables more effective optimizations.
	- Conditional constant propagation and global value numbering are more effective on SSA
- Program Dependence Graph (PDG) is useful in automatic parallelization, instruction scheduling, and software pipelining.

Machine-independent Code Optimization

- Intermediate code generation process introduces many inefficiencies
	- Extra copies of variables, using variables instead of constants, repeated evaluation of expressions etc.
- Code optimization removes such inefficiencies and improves code
- Improvement may be time, space, or power consumption
- It changes the structure of programs, sometimes of beyind recognition
	- Inlines functions, unrolls loops, eliminates some programmer-defined variables, etc.
- Code optimization consists of a bunch of heuristic and percentage of improvement depends on program 

Examples of Machine-Independent Optimizations

- Common sub-expression elimination
- Copy propagation
- Loop invariant code motion
- Partial redundancy elimination
- Induction variable elimination and strength reduction
- Code optimization needs information about the program
	- which expressions are being recomputed in a function?
	- which definitions reach a point?
- All such information is gathered through data-flow analysis

Code Generation

- Converts intermediate code to machine code
- Each intermediate code instruction may result in many machine instructions or vice-versa
- Must handle all aspects of machine architecture
	- Registers, pipelining, cache, multiple function units, etc.
- Generating efficient code is an NP-complete problem
	- Tree pattern matching-based strategies are among the best
	- Needs tree intermediate code
- Storage allocation decisions are made here
	- Register allocation and assignment are the most important problems

Machine-dependent Optimizations

- Peephole optimizations
	- Analyze sequence of instructinos in a small window (peephole) and using preset patterns, replace them with a more efficient sequence
	- Redundant instruction elimination
	- e.g. replace the sequence [LD, A,R1]ST R1,A] by [LD A,R1]
	- Eliminate "jump to jump" instructions
	- Use machine idioms (use INC instead of LD and ADD)
- Instruction scheduling (reordering) to eliminate pipelining interlocks and to increase parallelism
- Trace scheduling to increase the size of basic blocks and increase parallelism
- Software pipelining to increase parallelism in loops

